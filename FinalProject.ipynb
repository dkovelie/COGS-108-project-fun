{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 108 - Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important\n",
    "\n",
    "- ONE, and only one, member of your group should upload this notebook to TritonED. \n",
    "- Each member of the group will receive the same grade on this assignment. \n",
    "- Keep the file name the same: submit the file 'FinalProject.ipynb'.\n",
    "- Only upload the .ipynb file to TED, do not upload any associted data. Make sure that for cells in which you want graders to see output that these cells have been executed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group Members: Fill in the Student IDs of each group member here\n",
    "\n",
    "Replace the lines below to list each persons full student ID, ucsd email and full name.\n",
    "\n",
    "- A12814729, vkhua@ucsd.edu, Vivian Hua\n",
    "- A11983710, css003@ucsd.edu, Cody Sophearum Smith\n",
    "- U07979059, ddjiang@ucsd.edu, Derek De Gui Jiang\n",
    "- A13497348, allacuna@ucsd.edu, Allyson Llacuna\n",
    "- A12433857, Jor029@ucsd.edu, Joel Ramirez\n",
    "- A11774341, dkovelma@ucsd.edu, Daniel Kovelman-Ottilie\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start your project here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction and Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this project is to determine how do different regional factors influence unemployment rate in addition to attempting to come up with a model that could predict unemployment rate using strongly correlated factors. All correlations used for the formation of the unemployment rate prediction equation will be derived from a dataset containing various information from several different Wal-Mart stores within the United States. Wal-Mart data specfically will be used for the purposes of this project because Wal-Mart, as the \"largest private sector employer in the US,\" is often a \"lightning rod for criticism\" concerning issues of worker compensation, pay, and employment (1). In addition to being the largest private employer, research has also shown that Wal-Mart is so huge in the United States that there can even be implications for regional unemployment rates when a Wal-Mart moves into an area (2). Therefore, any correlations that can be found might be applicable and useful to other corporations wanting to open up another store in a specific area.\n",
    "\n",
    "\n",
    "Based upon previous findings on the relationship between unemployment rate and inflation(3) and to changes in gas prices(4), we hypothesized that factors such as comsumer price index (CPI), fuel price, and weekly sales will correlate strongly with unemployment rate with a negative, postive, and negative correlation respectively. Therefore, these will be the factors that will be included in the overall unemployment rate prediction equation. In addition, we hypothesized that other factors, such as average regional temerpature and holidays, would have a weak correlation with unemployment rate over time and will be dropped from the overall prediction equation due to negligble effects on unemployment.\n",
    "\n",
    "References (include links):\n",
    "- 1) Bhattarai, Abha, and Todd C. Frankel. \"Walmart Said It's Giving Its Employees A Raise. And Then It Closed 63 Stores.\" The Washington Post, 11 Jan. 2018, https://www.washingtonpost.com/news/business/wp/2018/01/11/walmart-to-raise-starting-hourly-wage-to-11-offer-paid-parental-leave/?fbclid=IwAR1L5UfHBA78aJ-vkee9tJZsBCpDlQyedVMNBrzrwdORjHCS-rz_LP1KMKk&noredirect=on&utm_term=.d01ffabc9bfa\n",
    "\n",
    "- 2) Keil, Stanley R. and Lee C. Spector. \"The Impact of Wal-Mart On Income And Unemployment Differentials In Alabama.\" The Review of Regional Studies 35.3 (2005): 336-355. Web. 16 Feb. 2019 https://pdfs.semanticscholar.org/1650/e455a3185c4b257b7b30c971c1606562ac31.pdf?fbclid=IwAR0ECEpANJRWRqDiftbjiqrdp_FseDbt2LIY4uzjM5pzgfi8TF-kwiGBPdk \n",
    "\n",
    "- 3) Picardo, Elvis. \"How Inflation and Unemployment Rate Are Related.\" Investopedia, 11 May 2018, https://www.investopedia.com/articles/markets/081515/how-inflation-and-unemployment-are-related.asp\n",
    "\n",
    "- 4) \"The Price Of Gas And The Unemployment Rate.\" Seeking Alpha, 14 Feb. 2011, https://seekingalpha.com/article/252704-the-price-of-gas-and-the-unemployment-rate?page=2&fbclid=IwAR2t9FILcjgUuubDZ8fAUdEhVRoc0ILlh35LnpJ4vRC7b5ar8e796JMWzMk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project, we used datasets found from a Kaggle competition (https://www.kaggle.com/c/walmart-recruiting-store-sales-forecasting/data). Of the datasets available, we used two of them, specifically features.csv and train.csv. The data span about 3 years of observations, and is entirely anonymized.\n",
    "\n",
    "features.csv details:\n",
    "features.csv contains 8190 observations, for data consisting of store number, date, temperature, fuel price, Markdown 1-5, CPI, unemployment, and IsHoliday. Store and date columns are strings and IsHoliday a boolean, the rest are floats.  \n",
    "\n",
    "train.csv details:\n",
    "train.csv contains 421,570 observations, for data consisting of store number, date, department, weekly sales, and IsHoliday. Store, date, and department are strings, IsHoliday a boolean, and weekly sales a float. \n",
    "\n",
    "Fuel price is in dollars, CPI is a ratio, temperature is in Fahrenheit, and unemployment is a percentage. \n",
    "\n",
    "For the analysis we performed, Markdown 1-5 and IsHoliday were not used, although IsHoliday was still retained in the dataset to aid in considering whether to remove outliers or not during the cleaning process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The imports for this project\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sp\n",
    "import matplotlib as mplot\n",
    "from collections import defaultdict\n",
    "from scipy import stats\n",
    "import patsy\n",
    "import statsmodels.api as sm\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import ttest_ind, chisquare, normaltest\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There were a few jobs to do to make the data usable for the types of analysis we desire. Since the datasets were made with a particular cause in mind, not all the data is relevant, as well as some of the observations are missing certain key data or are in a datatype not conducive to our analysis. \n",
    "To clean the data, we first removed columns not relevant to our analysis. The columns named Markdown 1-5 in features.csv were not be used in the analysis. To aid the analysis, we want all the relevant data in one dataset, and we want to use the weekly sales information from train.csv. The issue with train.csv was that not only are the observations split across store and date as with features.csv, but also across department. In order to add them to features, we first summed the weekly sales for each department on the same date in the same store, then we added that total weekly sales to the features dataset on store and date. Once that was accomplished, we converted the data in the \"Date\" column to be an integer instead of a string for plotting and analysis convenience. Lastly, we removed rows with missing unemployment data and checked for outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "weeksalesdb = pd.read_csv('train.csv')\n",
    "features = pd.read_csv('features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.drop(columns = ['MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converts weeksalesdb rows into dict and merging departments\n",
    "agg_sales = defaultdict(int)\n",
    "for ind, sale in weeksalesdb.iterrows():\n",
    "    agg_sales[str(sale['Store'])+'.'+sale['Date']] += sale['Weekly_Sales']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converts back into db with updated sales value\n",
    "storesales = pd.DataFrame(columns=['Store','Date','Weekly_Sales'])\n",
    "\n",
    "for key, value in agg_sales.items():\n",
    "    store, date = key.split('.')\n",
    "    newrow = [store, date, value]\n",
    "    storesales.loc[len(storesales)] = newrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to convert date to integer\n",
    "def convert_date(date):\n",
    "    date = date.strip()\n",
    "    date = date.replace('-','')\n",
    "    date = date.strip()\n",
    "    return int(date)\n",
    "\n",
    "#convert date columns\n",
    "storesales['Date'] = storesales['Date'].apply(convert_date)\n",
    "features['Date'] = features['Date'].apply(convert_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert store column to int (was str)\n",
    "storesales['Store'] = pd.to_numeric(storesales['Store'])\n",
    "features.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exported csv files\n",
    "storesales.to_csv('storesales.csv')\n",
    "features.to_csv('features2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merged two dataframes together on the store and date column\n",
    "merged = pd.merge(storesales, features, on =['Store', 'Date'], how = 'outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removed rows where unemployment data was empty\n",
    "merged.dropna(subset = ['Unemployment'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export cleaned features\n",
    "#merged.to_csv('features_clean.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checked for outliers in the weekly sales column, most of these are on or near holidays.\n",
    "outliers = merged[merged['Weekly_Sales'] > merged['Weekly_Sales'].mean() + 3 * merged['Weekly_Sales'].std()]\n",
    "outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.read_csv('features_clean.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.plot.scatter(x='Temperature', y='Unemployment')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While initially hypothesized that temperature would not be related, it is interesting to note that at the higher end of temperature, there are clusters of higher unemployment in the higher temperatures that are not present at lower temperatures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features['Temperature'].plot.hist()\n",
    "mplot.pyplot.xlabel('Temperature')\n",
    "mplot.pyplot.ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There was a concern that temperature's distribution may be problematic for analysis, but it appears normally distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.plot.scatter(x='CPI', y='Unemployment')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CPI appears to have a bimodal distribution in the dataset, which is further supported in the histogram below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features['CPI'].plot.hist()\n",
    "mplot.pyplot.xlabel('CPI')\n",
    "mplot.pyplot.ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.plot.scatter(x='Weekly_Sales', y='Unemployment')\n",
    "# Convert large values into log base 2 for easier data visualization\n",
    "# Weekly_sales is measured in dollars\n",
    "mplot.pyplot.xscale('log', basex=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weekly sales have a huge range of values, so this graph's x-axis is on a log base 2 scale. The interesting part of this visualization is the extreme high end of weekly sales values having lower unemployment rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features['Weekly_Sales'].plot.hist()\n",
    "mplot.pyplot.xlabel('Weekly_Sales')\n",
    "mplot.pyplot.ylabel('Frequency')\n",
    "mplot.pyplot.xscale('log', basex=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few very extreme sales values, but these days, when cleaning the data and looking at the outliers, tended to coincide with major holidays, which are arguably still representative of important data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.plot.scatter(x='Fuel_Price', y='Unemployment')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above graph, there appears to be a huge spread of data, which appears to be weakly correlated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features['Fuel_Price'].plot.hist()\n",
    "mplot.pyplot.xlabel('Fuel_Price')\n",
    "mplot.pyplot.ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis and Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\">Here, we will be analyzing how much each factor influences unemployment before developing a model.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_sales, pred_sales = patsy.dmatrices('Unemployment ~ Weekly_Sales', features)\n",
    "mod_sales = sm.OLS(out_sales, pred_sales)\n",
    "res_sales = mod_sales.fit()\n",
    "\n",
    "out_temp, pred_temp = patsy.dmatrices('Unemployment ~ Temperature', features)\n",
    "mod_temp = sm.OLS(out_temp, pred_temp)\n",
    "res_temp = mod_temp.fit()\n",
    "\n",
    "out_fuel, pred_fuel = patsy.dmatrices('Unemployment ~ Fuel_Price', features)\n",
    "mod_fuel = sm.OLS(out_fuel, pred_fuel)\n",
    "res_fuel = mod_fuel.fit()\n",
    "\n",
    "out_cpi, pred_cpi = patsy.dmatrices('Unemployment ~ CPI', features)\n",
    "mod_cpi = sm.OLS(out_cpi, pred_cpi)\n",
    "res_cpi = mod_cpi.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res_sales.summary(), res_temp.summary(), res_fuel.summary(), res_cpi.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\">After finding which features were most correlated, we attempted to build a model to predict unemployment from a combination of features, first using the ones that appeared most relevant.</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out1, pred1 = patsy.dmatrices('Unemployment ~ Weekly_Sales + CPI', features)\n",
    "mod1 = sm.OLS(out1, pred1)\n",
    "res1 = mod1.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res1.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\">To check what happens when all the factors are incorporated, we made an additional model with all the features.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out2, pred2 = patsy.dmatrices('Unemployment ~ Weekly_Sales + CPI + Fuel_Price + Temperature', features)\n",
    "mod2 = sm.OLS(out2, pred2)\n",
    "res2 = mod2.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res2.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\">The predictive power of the model went up when incorporating the additional features, as well as in the summary all factors are found to be significant (P>|t|). To further investigate the relationship between the features and unemployment, we standardize the data below.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardizevalue(self, df, label):\n",
    "    df = df.copy(deep=True)\n",
    "    series = df.loc[:, label]\n",
    "    avg = series.mean()\n",
    "    stdv = series.std()\n",
    "    series_standardized = (series - avg)/stdv\n",
    "    return series_standardized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#columns we want to standardize\n",
    "numericcolumns = features[['Weekly_Sales', 'Temperature', 'Fuel_Price', 'CPI', 'Unemployment']]\n",
    "#get the column names\n",
    "names = numericcolumns.columns\n",
    "#create scaler\n",
    "scaler = preprocessing.StandardScaler()\n",
    "#apply transformation\n",
    "normaled = scaler.fit_transform(numericcolumns)\n",
    "normaled = pd.DataFrame(normaled, columns=names)\n",
    "#delete the columns to be replaced with new values\n",
    "features_normal = features.drop(labels = names, axis = 'columns')\n",
    "#add in the columns from the normalized df\n",
    "features_normal[names] = normaled\n",
    "#rearrange columns to be like original features\n",
    "features_normal = features_normal[features.columns]\n",
    "#export csv file\n",
    "features_normal.to_csv('features_normal.csv', index = False)\n",
    "\n",
    "features_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out3, pred3 = patsy.dmatrices('Unemployment ~ Weekly_Sales + CPI + Fuel_Price + Temperature', features_normal)\n",
    "mod3 = sm.OLS(out3, pred3)\n",
    "res3 = mod3.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res3.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"4\">With standardization, it is clearer to see the impact each feature has on unemployment. CPI is the most strongly correlated, followed (surprisingly) by temperature, then weekly sales, and then fuel price.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Privacy/Ethics Concerns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned before, the data about Walmart was found on Kaggle, a public space where datasets can be accessible to those with the link to that information and an account. Since most of these are being used for competition purposes, the purpose of providing the data was to have anyone analyze it, meaning that the public was free to download the data with no restrictions nor hold additional responsibilities of it. The owner of the data, however, was not provided in Kaggle and makes its credibility questionable. Not knowing who provided the data and how they retrieved such information raises questions such as “how much of the data given was extracted from the original source?” and “how accurate is this information?” \n",
    "\n",
    "In concern with the actual data, our choice of datasets has not changed since the proposal. The data itself is extremely anonymized, making it nearly impossible to determine the identities of any of the stores in question, as the regions for each store are not specified nor the stores named anything beyond Store 1 through 45. The closest one could narrow down the stores is by comparing the temperature for each day for each store against weather reports for those days to determine an area the store could be in. Even if the identity of the store were found, there is no other data that could implicate anything about any individuals working or shopping there. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions and Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the analysis, we found that all four of the main factors analyzed were significant in predicting unemployment. In line with our hypothesis, CPI, weekly sales, and fuel price did have negative correlations with unemployment rate. Where we were surprised, however, was to find that temperature did have a significant correlation with unemployment in our model, and a positive one, at that. \n",
    "\n",
    "Given background research on how CPI and fuel price impact unemployment rate, the results we found are aligned with these findings. Weekly sales intuitively makes sense as with an increase in weekly sales, there would be a higher need for workers as those are busier times for the store, leading to a negative relationship with unemployment. Temperature may have to do with seasonal changes in demand. In the data visualization section, while temperature and unemployment do not appear to have a strong correlation initially, there is a cluster of higher unemployment rates when the temperature is high, which may indicate that unemployment is lower during the colder seasons. This may be due to how certain major holidays that are important for retail stores like Christmas or Thanksgiving are during the colder times of the year. \n",
    "\n",
    "Using the coefficients found in the model, we can see how much a change in a factor can influence unemployment. For example, a 10 million dollar increase in weekly sales would lead to about a 4 percent decrease in unemployment. Alternatively, to get a 1 percent decrease in unemployment, you would need an increase in more than 2.503 million dollars. Math shown below.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "10000000*-3.995e-07 #10 million (dollars) times coefficient for weekly sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-1/-3.995e-07 #reverse process to find how much change in dollars to get a 1% decrease in unemployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While factors such as CPI or temperature are not factors that can be readily altered by corporations to produce an effect, this information can be used to educate where, when, or how a company should expand its operations. Temperature, as mentioned before, being correlated with unemployment may be due to seasonal demand for work, which a corporation would definitely need to consider when hiring in different times of year in terms of how strong there is a supply and demand for labor, impacting compensation, benefits, or other factors that could influence the cost of operation. Regional unemployment itself can also be extremely important for companies to consider! Companies like Wal-Mart have pressures on them due to the low unemployment rate in recent years (5). Given that features such as temperature, CPI, fuel price, and weekly sales are correlated with unemployment in our model, data from those for a region can help a company determine whether opening or maintaining its current operations in that region are worthwhile or too costly. \n",
    "\n",
    "References\n",
    "- 5) Taylor, Kate. \"The Unemployment Rate Has Fallen To A 48-Year Low, And It's Terrifying News For Walmart, McDonald's, And JCPenney.\" Business Insider, 5 Oct. 2018, https://www.businessinsider.com/unemployment-rate-sparks-hiring-concerns-2018-10?fbclid=IwAR2PW52Drh-x60tGPTKNiZauPHMSwv9qjAQqdLWDlkW06uDcsgoxOUGvB3w"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
